在训练集中数据和标签已知的情况下，输入测试数据，将测试数据的特征与训练集中对应的特征进行相互比较，找到训练集中与之最为相似的前K个数据，则该测试数据对应的类别就是K个数据中出现次数最多的那个分类，其算法的描述为：

1）计算测试数据与各个训练数据之间的距离；（多用欧氏距离）

2）按照距离的递增关系进行排序；

3）选取距离最小的K个点；

4）确定前K个点所在类别的出现频率，以投票法；

5）返回前K个点中出现频率最高的类别作为测试数据的预测分类。


优点 
（1）简单，易于理解，易于实现，无需参数估计，无需训练;。
（2）可用于连续型随机变量和离散型随机变量------可分类可回归。
（3）精度高，理论成熟，对异常值不敏感（个别噪音数据对结果的影响不是很大）; 
（4）特别适合于多分类问题(multi-modal,对象具有多个类别标签)，KNN要比SVM表现要好. 
缺点 
（1）对测试样本分类时的计算量大，空间开销大，因为对每一个待分类的文本都要计算它到全体已知样本的距离，才能求得它的K个最近邻点。
目前常用的解决方法是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本; 
（2）可解释性差，无法给出决策树那样的规则; 
（3）最大的缺点是当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，
该样本的K个邻居中大容量类的样本占多数。该算法只计算“最近的”邻居样本，某一类的样本数量很大，
那么或者这类样本并不接近目标样本，或者这类样本很靠近目标样本。无论怎样，数量并不能影响运行结果。
可以采用权值的方法（和该样本距离小的邻居权值大）来改进; 
